#!/usr/bin/env python3
"""
ë¹ ë¥¸ INT8 TFLite ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸
- Ultralytics ë‚´ì¥ ê¸°ëŠ¥ ì‚¬ìš©
- ë³µì¡í•œ ë³€í™˜ ê³¼ì • ìƒëµ
"""

import os
import sys
from pathlib import Path

def quick_int8_conversion(model_path: str, output_dir: str):
    """Ultralyticsë¥¼ ì´ìš©í•œ ë¹ ë¥¸ INT8 ë³€í™˜"""
    print(f"ğŸš€ ë¹ ë¥¸ INT8 ë³€í™˜ ì‹œì‘: {model_path}")

    try:
        from ultralytics import YOLO

        # 1. ëª¨ë¸ ë¡œë“œ
        print("ğŸ“¥ YOLO ëª¨ë¸ ë¡œë“œ ì¤‘...")
        model = YOLO(model_path)

        # 2. INT8 TFLite ì§ì ‘ ë³€í™˜ (Ultralytics ìµœì í™” ì‚¬ìš©)
        print("âš¡ INT8 TFLite ë³€í™˜ ì‹¤í–‰...")

        # Ultralyticsì˜ ë‚´ì¥ INT8 ì˜µì…˜ ì‚¬ìš©
        export_path = model.export(
            format='tflite',
            imgsz=640,
            int8=True,  # INT8 ì–‘ìí™” í™œì„±í™”
            data='coco128.yaml',  # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ìš© ë°ì´í„°ì…‹
            batch=1
        )

        print(f"âœ… Ultralytics INT8 ë³€í™˜ ì™„ë£Œ: {export_path}")

        # 3. ê²°ê³¼ íŒŒì¼ì„ ëª©ì  ë””ë ‰í† ë¦¬ë¡œ ì´ë™
        if export_path and os.path.exists(export_path):
            import shutil
            model_name = Path(model_path).stem
            final_path = os.path.join(output_dir, f"{model_name}_ultralytics_int8.tflite")
            shutil.copy2(export_path, final_path)

            # íŒŒì¼ í¬ê¸° í™•ì¸
            size_mb = os.path.getsize(final_path) / (1024 * 1024)
            print(f"ğŸ“Š ìƒì„±ëœ INT8 ëª¨ë¸ í¬ê¸°: {size_mb:.2f} MB")

            return final_path
        else:
            print("âŒ ë³€í™˜ëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None

    except Exception as e:
        print(f"âŒ Ultralytics INT8 ë³€í™˜ ì‹¤íŒ¨: {e}")
        return None

def direct_pytorch_int8(model_path: str, output_dir: str):
    """PyTorch ëª¨ë¸ì„ ì§ì ‘ INT8ë¡œ ì–‘ìí™”"""
    print(f"ğŸ”§ PyTorch ì§ì ‘ INT8 ì–‘ìí™”: {model_path}")

    try:
        import torch
        import torch.quantization as quant

        # 1. ëª¨ë¸ ë¡œë“œ
        model = torch.load(model_path, map_location='cpu', weights_only=False)
        if isinstance(model, dict) and 'model' in model:
            model = model['model']
        model.eval()

        # 2. ì •ì  ì–‘ìí™” (ë” ê°•ë ¥í•œ INT8)
        print("âš¡ ì •ì  INT8 ì–‘ìí™” ì‹¤í–‰...")

        # QConfig ì„¤ì •
        model.qconfig = torch.quantization.get_default_qconfig('fbgemm')

        # ì–‘ìí™” ì¤€ë¹„
        model_prepared = torch.quantization.prepare(model)

        # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (ê°„ë‹¨í•œ ë”ë¯¸ ë°ì´í„°)
        print("ğŸ“Š ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹¤í–‰...")
        with torch.no_grad():
            for _ in range(10):  # ë¹ ë¥¸ ìº˜ë¦¬ë¸Œë ˆì´ì…˜
                dummy_input = torch.randn(1, 3, 640, 640)
                model_prepared(dummy_input)

        # INT8 ë³€í™˜
        model_int8 = torch.quantization.convert(model_prepared)

        # ì €ì¥
        model_name = Path(model_path).stem
        output_path = os.path.join(output_dir, f"{model_name}_pytorch_int8.pt")
        torch.save(model_int8, output_path)

        # í¬ê¸° ë¹„êµ
        original_size = os.path.getsize(model_path) / (1024 * 1024)
        quantized_size = os.path.getsize(output_path) / (1024 * 1024)
        reduction = ((original_size - quantized_size) / original_size) * 100

        print(f"âœ… PyTorch INT8 ì–‘ìí™” ì™„ë£Œ: {output_path}")
        print(f"ğŸ“Š í¬ê¸° ê°ì†Œ: {original_size:.2f} MB â†’ {quantized_size:.2f} MB ({reduction:.1f}% ê°ì†Œ)")

        return output_path

    except Exception as e:
        print(f"âŒ PyTorch INT8 ì–‘ìí™” ì‹¤íŒ¨: {e}")
        return None

def simple_onnx_int8(onnx_path: str, output_dir: str):
    """ONNX ëª¨ë¸ì„ ê°„ë‹¨í•˜ê²Œ INT8 ì–‘ìí™”"""
    print(f"ğŸ”§ ONNX INT8 ì–‘ìí™”: {onnx_path}")

    try:
        from onnxruntime.quantization import quantize_static, CalibrationDataReader
        import numpy as np

        # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° ìƒì„±
        class SimpleCalibrationDataReader(CalibrationDataReader):
            def __init__(self):
                self.data_list = []
                # 10ê°œì˜ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ìƒ˜í”Œë§Œ ì‚¬ìš© (ë¹ ë¦„)
                for _ in range(10):
                    data = np.random.randn(1, 3, 640, 640).astype(np.float32)
                    self.data_list.append({'input': data})
                self.iterator = iter(self.data_list)

            def get_next(self):
                try:
                    return next(self.iterator)
                except StopIteration:
                    return None

        # INT8 ì •ì  ì–‘ìí™”
        model_name = Path(onnx_path).stem
        output_path = os.path.join(output_dir, f"{model_name}_static_int8.onnx")

        calibration_reader = SimpleCalibrationDataReader()

        print("âš¡ ONNX ì •ì  INT8 ì–‘ìí™” ì‹¤í–‰...")
        quantize_static(
            onnx_path,
            output_path,
            calibration_reader,
            quant_format='IntegerOps',
            activation_type='int8',
            weight_type='int8'
        )

        # í¬ê¸° ë¹„êµ
        original_size = os.path.getsize(onnx_path) / (1024 * 1024)
        quantized_size = os.path.getsize(output_path) / (1024 * 1024)
        reduction = ((original_size - quantized_size) / original_size) * 100

        print(f"âœ… ONNX INT8 ì–‘ìí™” ì™„ë£Œ: {output_path}")
        print(f"ğŸ“Š í¬ê¸° ê°ì†Œ: {original_size:.2f} MB â†’ {quantized_size:.2f} MB ({reduction:.1f}% ê°ì†Œ)")

        return output_path

    except Exception as e:
        print(f"âŒ ONNX INT8 ì–‘ìí™” ì‹¤íŒ¨: {e}")
        return None

def main():
    """ëª¨ë“  INT8 ì–‘ìí™” ë°©ë²• ì‹œë„"""
    print("ğŸš€ ë¹ ë¥¸ INT8 ì–‘ìí™” ì‹œì‘")

    # ì…ë ¥/ì¶œë ¥ ë””ë ‰í† ë¦¬
    models_dir = "models"
    output_dir = "models/optimized"
    os.makedirs(output_dir, exist_ok=True)

    # ëª¨ë¸ ì°¾ê¸°
    model_files = []
    for ext in ['.pt', '.pth']:
        model_files.extend(Path(models_dir).glob(f"*{ext}"))

    onnx_files = list(Path(output_dir).glob("*.onnx"))

    if not model_files and not onnx_files:
        print("âŒ ë³€í™˜í•  ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return

    results = []

    # 1. Ultralytics INT8 ì‹œë„ (ê°€ì¥ ë¹ ë¦„)
    for model_path in model_files:
        print(f"\n{'='*50}")
        print(f"ğŸ¯ {model_path.name} - Ultralytics INT8 ë³€í™˜")

        result = quick_int8_conversion(str(model_path), output_dir)
        if result:
            results.append(("Ultralytics INT8", result))
            break  # ì„±ê³µí•˜ë©´ ë‹¤ë¥¸ ë°©ë²• ìƒëµ

    # 2. PyTorch ì§ì ‘ INT8 (ë¹ ë¦„)
    if not results:
        for model_path in model_files:
            print(f"\n{'='*50}")
            print(f"ğŸ¯ {model_path.name} - PyTorch ì§ì ‘ INT8")

            result = direct_pytorch_int8(str(model_path), output_dir)
            if result:
                results.append(("PyTorch INT8", result))
                break

    # 3. ONNX INT8 (ì¤‘ê°„ ì†ë„)
    for onnx_path in onnx_files:
        if 'quantized' not in onnx_path.name:  # ì´ë¯¸ ì–‘ìí™”ëœ ê²ƒ ì œì™¸
            print(f"\n{'='*50}")
            print(f"ğŸ¯ {onnx_path.name} - ONNX INT8 ì–‘ìí™”")

            result = simple_onnx_int8(str(onnx_path), output_dir)
            if result:
                results.append(("ONNX INT8", result))
                break

    # ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*50}")
    print("ğŸ‰ INT8 ì–‘ìí™” ê²°ê³¼ ìš”ì•½:")

    if results:
        for method, path in results:
            size_mb = os.path.getsize(path) / (1024 * 1024)
            print(f"âœ… {method}: {Path(path).name} ({size_mb:.2f} MB)")

        print(f"\nğŸ“ ê²°ê³¼ ìœ„ì¹˜: {output_dir}/")
        print("ğŸš€ Dockerì—ì„œ í…ŒìŠ¤íŠ¸: docker-compose up simple-evaluator")
    else:
        print("âŒ ëª¨ë“  INT8 ë³€í™˜ ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
        print("ğŸ’¡ TensorFlow ë²„ì „ í™•ì¸: pip install tensorflow==2.19.0")

if __name__ == "__main__":
    main()